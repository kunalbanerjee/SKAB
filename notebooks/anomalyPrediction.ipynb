{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e331c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = 'Kunal Banerjee'\n",
    "__date__ = '20240916'\n",
    "__email__ = 'kunal.banerjee.cse@gmail.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8883efd9",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b132f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier, XGBRegressor, plot_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, \\\n",
    "                            mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80e6ebf",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e014f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ecc4cd",
   "metadata": {},
   "source": [
    "## Preprocess the data for classification and regression tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f44a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(filename, retainDatetime=False, applyRegressionOnDroppingEdge=False, \n",
    "                    updateTarget='equal', num_steps=10):\n",
    "    print('Prepocessing ', filename)\n",
    "    data = pd.read_csv(filename, sep=';')\n",
    "    #print(data.columns)\n",
    "    \n",
    "    if retainDatetime == False:\n",
    "        data = data.drop(['datetime'], axis=1)\n",
    "        \n",
    "    data_label = data['anomaly']\n",
    "    data_label_regr = data_label.copy()\n",
    "    # Find the indices of the changepoints\n",
    "    index_cp = data[data['changepoint'] == 1].index\n",
    "    \n",
    "    if len(index_cp) != 4 and len(index_cp) != 2:\n",
    "        print('ERROR:: Unexpected number of changepoints ', len(index_cp), ' detected in ', filename)\n",
    "        exit()\n",
    "        \n",
    "    if len(index_cp) == 2:\n",
    "        print('Abrupt anomalies detected in ', filename)\n",
    "    else:\n",
    "        data_label[index_cp[0]:index_cp[1]] = 0\n",
    "        data_label[index_cp[2]:index_cp[3]] = 0\n",
    "        \n",
    "        if 'equal' == updateTarget:\n",
    "            for i in range(index_cp[0], index_cp[1]):\n",
    "                data_label_regr.loc[i] = (i - index_cp[0] + 1)/(index_cp[1] - index_cp[0] + 1)\n",
    "            if applyRegressionOnDroppingEdge:\n",
    "                for i in range(index_cp[2], index_cp[3]):\n",
    "                    # For the dropping edge, the target value for regression should be monotonically decreasing\n",
    "                    data_label_regr.loc[i] = 1.0 - ((i - index_cp[2] + 1)/(index_cp[3] - index_cp[2] + 1))\n",
    "        elif 'exponential' == updateTarget:\n",
    "            for i in range(index_cp[0], index_cp[1]):\n",
    "                data_label_regr.loc[i] = 1.0/pow(2,(index_cp[1] - i))\n",
    "            if applyRegressionOnDroppingEdge:\n",
    "                for i in range(index_cp[2], index_cp[3]):\n",
    "                    # For the dropping edge, the target value for regression should be monotonically decreasing\n",
    "                    data_label_regr.loc[i] = 1.0/pow(2,(i - index_cp[2]))\n",
    "        elif 'step' == updateTarget:\n",
    "            total_range_ascend = index_cp[1] - index_cp[0]\n",
    "            incr_value = 1.0/num_steps\n",
    "            for i in range(index_cp[0], index_cp[1]):\n",
    "                data_label_regr.loc[i] = ((i - index_cp[0])//(total_range_ascend/num_steps))*incr_value\n",
    "            if applyRegressionOnDroppingEdge:\n",
    "                total_range_descend = index_cp[2] - index_cp[1]\n",
    "                for i in range(index_cp[2], index_cp[3]):\n",
    "                    # For the dropping edge, the target value for regression should be monotonically decreasing\n",
    "                    data_label_regr.loc[i] = ((index_cp[3] - i)//(total_range_descend/num_steps))*incr_value\n",
    "                        \n",
    "    # Drop the target variables\n",
    "    data = data.drop(['anomaly', 'changepoint'], axis=1)\n",
    "    \n",
    "    return data, data_label, data_label_regr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce1757e",
   "metadata": {},
   "source": [
    "## Perform classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da3c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(train, train_label, test, test_label, algorithm='xgb'):\n",
    "    # create model instance\n",
    "    if algorithm == 'xgb':\n",
    "        classifier = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n",
    "    elif algorithm == 'svm':\n",
    "        classifier = SVC(kernel='linear')\n",
    "    elif algorithm == 'gaussiannb':\n",
    "        classifier = GaussianNB()\n",
    "    else:\n",
    "        print('ERROR:: Unknown classification algorithm: ', algorithm)\n",
    "        exit()\n",
    "    # fit model\n",
    "    classifier.fit(train, train_label)\n",
    "    # make predictions\n",
    "    preds = classifier.predict(test)\n",
    "    return classifier, accuracy_score(test_label, preds), balanced_accuracy_score(test_label, preds), \\\n",
    "           f1_score(test_label, preds), precision_score(test_label, preds), recall_score(test_label, preds)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b00a8a3",
   "metadata": {},
   "source": [
    "## Perform regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2120f5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(train, train_label, test, test_label, algorithm='xgb'):\n",
    "    # create model instance\n",
    "    if algorithm == 'xgb':\n",
    "        regressor = XGBRegressor(objective ='reg:squarederror', n_estimators = 10, seed = 123)\n",
    "    elif algorithm == 'svm':\n",
    "        regressor = SVR(kernel='rbf')\n",
    "    elif algorithm == 'lasso':\n",
    "        regressor = Lasso(alpha=0.1)\n",
    "    elif algorithm == 'ridge':\n",
    "        regressor = Ridge(alpha=0.1)\n",
    "    else:\n",
    "        print('ERROR:: Unknown regression algorithm: ', algorithm)\n",
    "        exit()\n",
    "    # fit model\n",
    "    regressor.fit(train, train_label)\n",
    "    # make predictions\n",
    "    preds = regressor.predict(test)\n",
    "    rmse = mean_squared_error(test_label, preds, squared=True)\n",
    "    return regressor, rmse\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aecfb22",
   "metadata": {},
   "source": [
    "## Perform anomaly prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352017bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_prediction(data, classifier, regressor):\n",
    "    preds = list()\n",
    "    for row in data:\n",
    "        row = row.reshape(1,-1)\n",
    "        pred = classifier.predict(row)\n",
    "        if pred != 1:\n",
    "            pred = regressor.predict(row)\n",
    "        \n",
    "        preds.append(pred)\n",
    "        \n",
    "    return preds\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd29e622",
   "metadata": {},
   "source": [
    "## Evaluate anomaly prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b6eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_anomaly_prediction(test_label_regr, preds, threshold):\n",
    "    # Expecting the test label to contain values between 0 and 1, i.e., it's same as test_label_regr\n",
    "    for i in range(len(test_label_regr)):\n",
    "        if test_label_regr[i] != 0:\n",
    "            test_label_regr.loc[i] = 1\n",
    "    \n",
    "    # We need to create a copy so that the original preds does not get modified\n",
    "    predsCopy = list()\n",
    "    \n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] >= threshold:\n",
    "            predsCopy.append(1)\n",
    "        else:\n",
    "            predsCopy.append(0)\n",
    "            \n",
    "    return accuracy_score(test_label_regr, predsCopy), balanced_accuracy_score(test_label_regr, predsCopy), \\\n",
    "           f1_score(test_label_regr, predsCopy), precision_score(test_label_regr, predsCopy), \\\n",
    "           recall_score(test_label_regr, predsCopy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d8e454",
   "metadata": {},
   "source": [
    "## Code for training and testing anomaly prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe03bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test():\n",
    "    # Create the list of files to be used for training\n",
    "    trainFiles = list()\n",
    "    for i in range(3):\n",
    "        filename = PATH_TO_DATA + 'valve2/' + str(i) + '.csv'\n",
    "        trainFiles.append(filename)\n",
    "    \n",
    "    print('Files used for training: ', trainFiles)\n",
    "    \n",
    "    # Create the list of files to be used for testing\n",
    "    testFiles = list()\n",
    "    for i in range(3,4):\n",
    "        filename = PATH_TO_DATA + 'valve2/' + str(i) + '.csv'\n",
    "        testFiles.append(filename)\n",
    "        \n",
    "    print('Files used for testing: ', testFiles)\n",
    "    \n",
    "    # We don't need to retain datetime for classifiers such as XGBClassifier\n",
    "    retainDatetime=False\n",
    "    \n",
    "    columns = ['Accelerometer1RMS', 'Accelerometer2RMS', 'Current', 'Pressure', 'Temperature', \n",
    "               'Thermocouple', 'Voltage', 'Volume Flow RateRMS']\n",
    "    if retainDatetime:\n",
    "        columns = columns.insert(0, 'datetime')\n",
    "\n",
    "    train = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    train_label = pd.Series(dtype=np.float64)\n",
    "    train_label_regr = pd.Series(dtype=np.float64)\n",
    "    for file in trainFiles:\n",
    "        data, data_label, data_label_regr = preprocess_data(file)\n",
    "        train = pd.concat([train, data], ignore_index=True)\n",
    "        train_label = pd.concat([train_label, data_label], ignore_index=True)\n",
    "        train_label_regr = pd.concat([train_label_regr, data_label_regr], ignore_index=True)\n",
    "        \n",
    "    test = pd.DataFrame(columns=columns)\n",
    "    test_label = pd.Series(dtype=np.float64)\n",
    "    test_label_regr = pd.Series(dtype=np.float64)\n",
    "    for file in testFiles:\n",
    "        data, data_label, data_label_regr = preprocess_data(file)\n",
    "        test = pd.concat([test, data], ignore_index=True)\n",
    "        test_label = pd.concat([test_label, data_label], ignore_index=True)\n",
    "        test_label_regr = pd.concat([test_label_regr, data_label_regr], ignore_index=True)\n",
    "    \n",
    "    # Scale the data\n",
    "    # Note that due to scaling, pandas.DataFrame becomes numpy.ndarray\n",
    "    scaler = StandardScaler()\n",
    "    if retainDatetime == False:\n",
    "        scaler.fit(train)\n",
    "        train = scaler.transform(train)\n",
    "        test = scaler.transform(test)\n",
    "    else:\n",
    "        cols_to_scale = columns\n",
    "        cols_to_scale.remove('datetime')\n",
    "        scaler.fit(train[cols_to_scale])\n",
    "        train[cols_to_scale] = scaler.transform(train[cols_to_scale])\n",
    "        test[cols_to_scale] = scaler.transform(test[cols_to_scale])\n",
    "    \n",
    "    classifier, accuracy, balanced_accuracy, f1, precision, recall = \\\n",
    "        classification(train, train_label, test, test_label, algorithm='gaussiannb')\n",
    "    print('Classification:: Accuracy: ', accuracy, ' Balanced Accuracy: ', balanced_accuracy, \n",
    "          ' F1: ', f1, ' Precision: ', precision, ' Recall: ', recall)\n",
    "    \n",
    "    # Plot feature importance of classifier\n",
    "    #plot_importance(classifier)\n",
    "    #plt.show()\n",
    "    \n",
    "    regressor, rmse = regression(train, train_label_regr, test, test_label_regr, algorithm='ridge')\n",
    "    print('Regression:: RMSE: ', rmse)\n",
    "    \n",
    "    preds = anomaly_prediction(test, classifier, regressor)\n",
    "    print('Anomaly Prediction')\n",
    "    for threshold in np.arange(0.5, 1.0, 0.1):\n",
    "        APaccuracy, APbalanced_accuracy, APf1, APprecision, APrecall = \\\n",
    "            evaluate_anomaly_prediction(test_label_regr, preds, threshold)\n",
    "        print('Threshold:: ', threshold, ' Accuracy: ', APaccuracy, ' Balanced Accuracy: ', APbalanced_accuracy, \n",
    "              ' F1: ', APf1, ' Precision: ', APprecision, ' Recall: ', APrecall)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230eae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa16a078",
   "metadata": {},
   "source": [
    "## Pre-processing for stat-based anomaly prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f0bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_stat(filename, applyRegressionOnDroppingEdge=True):\n",
    "    print('Prepocessing for stat-based prediction', filename)\n",
    "    data = pd.read_csv(filename, sep=';')\n",
    "    #print(data.columns)\n",
    "    \n",
    "    # Drop datetime column\n",
    "    data = data.drop(['datetime'], axis=1)\n",
    "        \n",
    "    data_label = data['anomaly']\n",
    "    data_label_regr = data_label.copy()\n",
    "    # Find the indices of the changepoints\n",
    "    index_cp = data[data['changepoint'] == 1].index\n",
    "    \n",
    "    if len(index_cp) != 4 and len(index_cp) != 2:\n",
    "        print('ERROR:: Unexpected number of changepoints ', len(index_cp), ' detected in ', filename)\n",
    "        exit()\n",
    "        \n",
    "    if len(index_cp) == 2:\n",
    "        print('Abrupt anomalies detected in ', filename)\n",
    "    else:\n",
    "        data_label[index_cp[0]:index_cp[1]] = 0\n",
    "        data_label[index_cp[2]:index_cp[3]] = 0\n",
    "        \n",
    "        data_label_regr[index_cp[0]:index_cp[1]] = 1     \n",
    "        if applyRegressionOnDroppingEdge:\n",
    "            data_label_regr[index_cp[2]:index_cp[3]] = 1\n",
    "        else:\n",
    "            data_label_regr[index_cp[2]:index_cp[3]] = 0\n",
    "           \n",
    "    # Drop the target variables\n",
    "    data = data.drop(['anomaly', 'changepoint'], axis=1)\n",
    "    \n",
    "    return data, data_label, data_label_regr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec91dc41",
   "metadata": {},
   "source": [
    "## Evaluate stat-based anomaly prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03813ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_anomaly_prediction_stat(train, train_label, test, test_label):\n",
    "    train_mean = train.mean()\n",
    "    train_std = train.std()\n",
    "    best_f1 = 0.0\n",
    "    best_num_vars = 0\n",
    "    best_deviation = 0.0\n",
    "    # The variable num_vars controls the number of variables that should cross threshold for an anomaly\n",
    "    for num_vars in range(1, len(train.columns)+1):\n",
    "        # The variable deviation controls the standard deviation that needs to be crossed for an anomaly\n",
    "        for deviation in np.arange(1.0, 4.0, 0.1):\n",
    "            preds = list()\n",
    "            for index, row in train.iterrows():\n",
    "                vars_crossing_threshold = 0\n",
    "                for col in train.columns:\n",
    "                    if row[col] > (train_mean[col] + deviation * train_std[col]) or \\\n",
    "                       row[col] < (train_mean[col] - deviation * train_std[col]):\n",
    "                        vars_crossing_threshold += 1\n",
    "                if vars_crossing_threshold >= num_vars:\n",
    "                    preds.append(1)\n",
    "                else:\n",
    "                    preds.append(0)\n",
    "            \n",
    "            current_f1 = f1_score(train_label, preds)\n",
    "            if current_f1 > best_f1:\n",
    "                best_f1 = current_f1\n",
    "                best_num_vars = num_vars\n",
    "                best_deviation = deviation\n",
    "    print('Best training F1-score: ', best_f1)\n",
    "    print('Standard deviation corresponding to best F1-score: ', best_deviation)\n",
    "    print('Number of variables that should cross the thtreshold for the best F1-score: ', best_num_vars)\n",
    "                \n",
    "    preds = list()\n",
    "    for index, row in test.iterrows():\n",
    "        vars_crossing_threshold = 0\n",
    "        for col in test.columns:\n",
    "            if row[col] > (train_mean[col] + best_deviation * train_std[col]) or \\\n",
    "               row[col] < (train_mean[col] - best_deviation * train_std[col]):\n",
    "                vars_crossing_threshold += 1\n",
    "        if vars_crossing_threshold >= best_num_vars:\n",
    "            preds.append(1)\n",
    "        else:\n",
    "            preds.append(0)\n",
    "    \n",
    "    # This following line was added because otherwise the test_label_regr values were being read as\n",
    "    # real numbers leading to error while computing classification metrics\n",
    "    test_label = test_label.astype(int)\n",
    "    \n",
    "    return accuracy_score(test_label, preds), balanced_accuracy_score(test_label, preds), \\\n",
    "           f1_score(test_label, preds), precision_score(test_label, preds), recall_score(test_label, preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a5a31f",
   "metadata": {},
   "source": [
    "## Stat-based anomaly prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7abc88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_stat():\n",
    "    # Create the list of files to be used for training\n",
    "    trainFiles = list()\n",
    "    for i in range(3):\n",
    "        filename = PATH_TO_DATA + 'valve2/' + str(i) + '.csv'\n",
    "        trainFiles.append(filename)\n",
    "    \n",
    "    print('Files used for training: ', trainFiles)\n",
    "    \n",
    "    # Create the list of files to be used for testing\n",
    "    testFiles = list()\n",
    "    for i in range(3,4):\n",
    "        filename = PATH_TO_DATA + 'valve2/' + str(i) + '.csv'\n",
    "        testFiles.append(filename)\n",
    "        \n",
    "    print('Files used for testing: ', testFiles)\n",
    "    \n",
    "    # We don't need to retain datetime for stat-based anomaly detection and prediction\n",
    "    columns = ['Accelerometer1RMS', 'Accelerometer2RMS', 'Current', 'Pressure', 'Temperature', \n",
    "               'Thermocouple', 'Voltage', 'Volume Flow RateRMS']\n",
    "\n",
    "    train = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    train_label = pd.Series(dtype=np.float64)\n",
    "    train_label_regr = pd.Series(dtype=np.float64)\n",
    "    for file in trainFiles:\n",
    "        data, data_label, data_label_regr = preprocess_data_stat(file)\n",
    "        train = pd.concat([train, data], ignore_index=True)\n",
    "        train_label = pd.concat([train_label, data_label], ignore_index=True)\n",
    "        train_label_regr = pd.concat([train_label_regr, data_label_regr], ignore_index=True)\n",
    "        \n",
    "    test = pd.DataFrame(columns=columns)\n",
    "    test_label = pd.Series(dtype=np.float64)\n",
    "    test_label_regr = pd.Series(dtype=np.float64)\n",
    "    for file in testFiles:\n",
    "        data, data_label, data_label_regr = preprocess_data(file)\n",
    "        test = pd.concat([test, data], ignore_index=True)\n",
    "        test_label = pd.concat([test_label, data_label], ignore_index=True)\n",
    "        test_label_regr = pd.concat([test_label_regr, data_label_regr], ignore_index=True)\n",
    "    \n",
    "    accuracy, balanced_accuracy, f1, precision, recall = \\\n",
    "        evaluate_anomaly_prediction_stat(train, train_label, test, test_label)\n",
    "    print('Anomaly Datection:: Accuracy: ', accuracy, ' Balanced Accuracy: ', balanced_accuracy, \n",
    "          ' F1: ', f1, ' Precision: ', precision, ' Recall: ', recall)\n",
    "    \n",
    "    accuracy, balanced_accuracy, f1, precision, recall = \\\n",
    "        evaluate_anomaly_prediction_stat(train, train_label_regr, test, test_label_regr)\n",
    "    print('Anomaly Prediction:: Accuracy: ', accuracy, ' Balanced Accuracy: ', balanced_accuracy, \n",
    "          ' F1: ', f1, ' Precision: ', precision, ' Recall: ', recall)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabc9b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_stat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea82b9c9",
   "metadata": {},
   "source": [
    "## Anomaly prediction with shifted anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0df8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_shifted():\n",
    "    # Create the list of files to be used for training\n",
    "    trainFiles = list()\n",
    "    for i in range(3):\n",
    "        filename = PATH_TO_DATA + 'valve2/' + str(i) + '.csv'\n",
    "        trainFiles.append(filename)\n",
    "    \n",
    "    print('Files used for training: ', trainFiles)\n",
    "    \n",
    "    # Create the list of files to be used for testing\n",
    "    testFiles = list()\n",
    "    for i in range(3,4):\n",
    "        filename = PATH_TO_DATA + 'valve2/' + str(i) + '.csv'\n",
    "        testFiles.append(filename)\n",
    "        \n",
    "    print('Files used for testing: ', testFiles)\n",
    "    \n",
    "    # We don't need to retain datetime for classifiers such as XGBClassifier\n",
    "    retainDatetime=False\n",
    "    \n",
    "    columns = ['Accelerometer1RMS', 'Accelerometer2RMS', 'Current', 'Pressure', 'Temperature', \n",
    "               'Thermocouple', 'Voltage', 'Volume Flow RateRMS']\n",
    "    if retainDatetime:\n",
    "        columns = columns.insert(0, 'datetime')\n",
    "\n",
    "    train = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    train_label = pd.Series(dtype=np.float64)\n",
    "    train_label_regr = pd.Series(dtype=np.float64)\n",
    "    for file in trainFiles:\n",
    "        data, data_label, data_label_regr = preprocess_data_stat(file)\n",
    "        train = pd.concat([train, data], ignore_index=True)\n",
    "        train_label = pd.concat([train_label, data_label], ignore_index=True)\n",
    "        train_label_regr = pd.concat([train_label_regr, data_label_regr], ignore_index=True)\n",
    "        \n",
    "    test = pd.DataFrame(columns=columns)\n",
    "    test_label = pd.Series(dtype=np.float64)\n",
    "    test_label_regr = pd.Series(dtype=np.float64)\n",
    "    for file in testFiles:\n",
    "        data, data_label, data_label_regr = preprocess_data_stat(file)\n",
    "        test = pd.concat([test, data], ignore_index=True)\n",
    "        test_label = pd.concat([test_label, data_label], ignore_index=True)\n",
    "        test_label_regr = pd.concat([test_label_regr, data_label_regr], ignore_index=True)\n",
    "    \n",
    "    # Scale the data\n",
    "    # Note that due to scaling, pandas.DataFrame becomes numpy.ndarray\n",
    "    scaler = StandardScaler()\n",
    "    if retainDatetime == False:\n",
    "        scaler.fit(train)\n",
    "        train = scaler.transform(train)\n",
    "        test = scaler.transform(test)\n",
    "    else:\n",
    "        cols_to_scale = columns\n",
    "        cols_to_scale.remove('datetime')\n",
    "        scaler.fit(train[cols_to_scale])\n",
    "        train[cols_to_scale] = scaler.transform(train[cols_to_scale])\n",
    "        test[cols_to_scale] = scaler.transform(test[cols_to_scale])\n",
    "    \n",
    "    classifier, accuracy, balanced_accuracy, f1, precision, recall = \\\n",
    "        classification(train, train_label, test, test_label, algorithm='xgb')\n",
    "    print('Classification:: Accuracy: ', accuracy, ' Balanced Accuracy: ', balanced_accuracy, \n",
    "          ' F1: ', f1, ' Precision: ', precision, ' Recall: ', recall)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e915303",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_shifted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c26b815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
